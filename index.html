<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}

.btn-group button {
  background-color: orange;
  border: 1px solid #FF6C00;
  color: white; /* White text */
  padding: 5px 12px; /* Some padding */
  cursor: pointer; /* Pointer/hand icon */
  float: center; /* Float the buttons side by side */
}

/* Add a background color on hover */
.btn-group button:hover {
  background-color: #FF6C00;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>

  <meta name="theme-color" content="#ffffff" />


  <!--  Global site tag (gtag.js) - Google Analytics 
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141682504-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-141682504-1');
  </script>
  -->


  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Weight Agnostic Neural Networks" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="Weight Agnostic Neural Networks" />
  <meta name="twitter:image" content="https://weightagnostic.github.io/assets/img/wann_card_square_v2.png" />


  <!-- SEO -->
  <meta property="og:title" content="Weight Agnostic Neural Networks" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Networks that can already (sort of) perform tasks with random weights." />
  <meta property="og:image" content="https://weightagnostic.github.io/assets/img/wann_card_rect_v2.png" />
  <meta property="og:url" content="https://weightagnostic.github.io/" />



</head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<script type="text/front-matter">
  title: "Some Amazing Thing -- Autodesk Research"
  description: ""
</script>
<body>
<div style="text-align: center;">
<video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/mp4/wann_cover.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="50%"><figcaption style="text-align: center;">A weight agnostic neural network performing <i>BipedalWalker-v2</i> task at various different weight parameters.</figcaption></td>
</tr></table>

</div>

<dt-article id="dtbody">

<dt-byline class="l-page transparent"></dt-byline>
<h1>SAT-ID: Some Amazing Thing I Did</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=GGyARB8AAAAJ&hl=en">You</a>
        <a class="affiliation" href="https://autodeskresearch.com/">Autodesk Research</a>
    </div>
    <div class="author">
        <a class="name" href="http://blog.otoro.net/">Him</a>
        <a class="affiliation" href="https://autodeskresearch.com/">Autodesk Research</a>
    </div>
    <div class="author">
        <a class="name" href="http://blog.otoro.net/">Her</a>
        <a class="affiliation" href="https://autodeskresearch.com/">Autodesk Research</a>
    </div>
  </div>
  <div class="date">
    <div class="month">June 12</div>
    <div class="year">2019</div>
  </div>
  <div class="date">
    <div class="month">Download</div>
    <div class="year" style="color: #FF6C00;"><a href="https://arxiv.org/abs/1906.04358" target="_blank">PDF</a></div>
  </div>
</div>
</dt-byline>
</dt-byline>
<h2>Abstract</h2>
<p>Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On supervised learning domain, we find architectures that can achieve much higher than chance accuracy on MNIST using random weights.</p>
<hr>
<h2>Introduction</h2>
<p>In biology, precocial species are those whose young already possess certain abilities from the moment of birth <dt-cite key="bbc_islands"></dt-cite>. There is evidence to show that lizard <dt-cite key="miles1995morphological"></dt-cite> and snake <dt-cite key="burger1998antipredator,mori2000does"></dt-cite> hatchlings already possess behaviors to escape from predators. Shortly after hatching, ducks are able to swim and eat on their own <dt-cite key="starck1998patterns"></dt-cite>, and turkeys can visually recognize predators <dt-cite key="goth2001innate"></dt-cite>. In contrast, when we train artificial agents to perform a task, we typically choose a neural network architecture we believe to be suitable for encoding a policy for the task, and find the weight parameters of this policy using a learning algorithm. Inspired by precocial behaviors evolved in nature, in this work, we develop neural networks with architectures that are naturally capable of performing a given task even when their weight parameters are randomly sampled. By using such neural network architectures, our agents can already perform well in their environment without the need to learn weight parameters.</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/mp4/square_biped.mp4" type="video/mp4" autoplay muted playsinline loop style="width:50%;" ></video><video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/mp4/square_racer.mp4" type="video/mp4" autoplay muted playsinline loop style="width:50%;" ></video>
<br/><br/>
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/rl_cover_left.png" style="width: 50%;"/><img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/rl_cover_right.png" style="width: 50%;"/>
<br/>
<figcaption style="text-align: left;">
<b>Examples of Weight Agnostic Neural Networks: Bipedal Walker (left), Car Racing (right)</b><br/>
We search for architectures by deemphasizing weights. In place of training, networks are assigned a single shared weight value at each rollout. Architectures that are optimized for expected performance over a wide range of weight values are still able to perform various tasks without weight training.
</figcaption>
</div>
<!--<div style="text-align: center;">
<br/><br/>
<video class="b-lazy" data-src="assets/mp4/square_swingup.mp4" type="video/mp4" autoplay muted playsinline loop style="width:33.33%;" ></video><video class="b-lazy" data-src="assets/mp4/square_biped.mp4" type="video/mp4" autoplay muted playsinline loop style="width:33.33%;" ></video><video class="b-lazy" data-src="assets/mp4/square_racer.mp4" type="video/mp4" autoplay muted playsinline loop style="width:33.33%;" ></video>
<br/><br/>
<img class="b-lazy" src="assets/png/rl_cover_all.png" style="display: block; margin: auto; width: 100%;"/>
<br/>
<figcaption style="text-align: left;">
<b>(all three demos)</b><br/>
We search for architectures by deemphasizing weights. In place of training, networks are assigned a single shared weight value at each rollout. Architectures that are optimized for expected performance over a wide range of weight values are still able to perform various tasks without weight training.
</figcaption>
</div>-->
<p>Decades of neural network research have provided building blocks with strong inductive biases for various task domains. Convolutional networks <dt-cite key="lecun1995convolutional,fukushima1982neocognitron"></dt-cite> are especially suited for image processing <dt-cite key="cohen2016inductive"></dt-cite>. Recent work <dt-cite key="he2016powerful,ulyanov2018deep"></dt-cite> demonstrated that even randomly-initialized CNNs can be used effectively for image processing tasks such as superresolution, inpainting and style transfer. <dt-cite key="evolino">Schmidhuber et al.</dt-cite> have shown that a randomly-initialized LSTM <dt-cite key="lstm"></dt-cite> with a learned linear output layer can predict time series where traditional RNNs trained using reservoir methods <dt-cite key="jaeger2004harnessing,reservoir"></dt-cite> fail. More recent developments in self-attention <dt-cite key="vaswani2017attention"></dt-cite> and capsule <dt-cite key="sabour2017dynamic"></dt-cite> networks expand the toolkit of building blocks for creating architectures with strong inductive biases for various tasks. Fascinated by the intrinsic capabilities of randomly-initialized CNNs and LSTMs, we aim to search for <em>weight agnostic neural networks</em>, architectures with strong inductive biases that can already perform various tasks with random weights.</p>
<div style="text-align: center;">
<br/>
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/mnist_cover.png" style="display: block; margin: auto; width: 100%;"/>
<br/>
<figcaption style="text-align: left;">
<b>MNIST classification network evolved to work with random weights</b><br/>
<!--Networks that already work with random weight parameters are not only easily trainable, but as we will demonstrate, we can also use the same network architecture with an ensemble of different weights to increase performance, without the need to explicitly train the weight parameters.-->
Network architectures that already work with random weights are not only easily trainable, they also offer other advantages too. For instance, we can give the same network an ensemble of (untrained) weights to increase performance, without the need to explicitly train any weight parameters.
<br/><br/>
While a conventional network with random initialization will get ~ 10% accuracy on MNIST, this particular network architecture achieves a much better than chance accuracy on MNIST (> 80%) with random weights. Without any weight training, the accuracy increases to > 90% when we use an ensemble of untrained weights.
</figcaption>
</div>
<p>In order to find neural network architectures with strong inductive biases, we propose to search for architectures by deemphasizing the importance of weights. This is accomplished by <strong>(1)</strong> assigning a single shared weight parameter to every network connection and <strong>(2)</strong> evaluating the network on a wide range of this single weight parameter. In place of optimizing weights of a fixed network, we optimize instead for architectures that perform well over a wide range of weights. We demonstrate our approach can produce networks that can be expected to perform various continuous control tasks with a random weight parameter. As a proof of concept, we also apply our search method on a supervised learning domain, and find it can discover networks that, even without explicit weight training, can achieve a much higher than chance test accuracy of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 92% on MNIST. We hope our demonstration of such weight agnostic neural networks will encourage further research exploring novel neural network building blocks that not only possess useful inductive biases, but can also learn using algorithms that are not necessarily limited to gradient-based methods.</p>
<div style="text-align: center;">
<br/>
<div id="intro_demo" class="unselectable" style="text-align: center;"></div>
<br/>
<figcaption style="color:#FF6C00;">Interactive Demo</figcaption>
<figcaption style="text-align: left;">
A weight agnostic neural network performing <i>CartpoleSwingup</i> task. Drag the slider to control the weight parameter and observe the performance at various shared weight parameters. You can also fine-tune the individual weights of all connections in this demo.
</figcaption>
</div>
<hr>
<h2>A couple other examples</h2>
<p>You can use latex-like syntax for math:</p>
<p><strong>1.</strong>  <em>Random weights</em>:  individual weights drawn from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">U</mi></mrow><mo>(</mo><mo>−</mo><mn>2</mn><mo separator="true">,</mo><mn>2</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-2,2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.09931em;">U</span></span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">2</span><span class="mpunct">,</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span>.</p>
<p><strong>2.</strong>  <em>Random shared weight</em>:  a single shared weight drawn from <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">U</mi></mrow><mo>(</mo><mo>−</mo><mn>2</mn><mo separator="true">,</mo><mn>2</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-2,2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.09931em;">U</span></span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">2</span><span class="mpunct">,</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span>.</p>
<p><strong>3.</strong>  <em>Tuned shared weight</em>:  the highest performing shared weight value in range <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>−</mo><mn>2</mn><mo separator="true">,</mo><mn>2</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(-2,2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord">−</span><span class="mord mathrm">2</span><span class="mpunct">,</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span>.</p>
<p><strong>4.</strong>  <em>Tuned weights</em>:  individual weights tuned using population-based REINFORCE <dt-cite key="williams1992simple"></dt-cite>.</p>
<p>Here is a simple js bit for swapping through images (see end of lib/controller.js).</p>
<div style="text-align: center;">
<br/>
<img id="mnist_figure" src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/mnist_all.png" style="display: block; margin: auto; width: 100%;"/>
<div class="btn-group">
  <button id="mnist_all">All</button>&nbsp;
  <button id="mnist_0">0</button>&nbsp;
  <button id="mnist_1">1</button>&nbsp;
  <button id="mnist_2">2</button>&nbsp;
  <button id="mnist_3">3</button>&nbsp;
  <button id="mnist_4">4</button>&nbsp;
  <button id="mnist_5">5</button>&nbsp;
  <button id="mnist_6">6</button>&nbsp;
  <button id="mnist_7">7</button>&nbsp;
  <button id="mnist_8">8</button>&nbsp;
  <button id="mnist_9">9</button>
</div>
<br/>
<figcaption style="color:#FF6C00;">Interactive Demo</figcaption>
<figcaption style="text-align: left;">
<b>MNIST Classifier</b><br/>
Not all neurons and connections are used to predict each digit. Starting from the output connection for a particular digit, we can map out the part of the network used to classify that digit. We can also see which parts of the inputs are used for classification.
</figcaption>
</div>
<p><em>If you would like to discuss any issues or give feedback, please visit the <a href="https://github.com/weightagnostic/weightagnostic.github.io/issues">GitHub</a> repository of this page for more information.</em></p>
</dt-article>
<dt-appendix>
<h2>Acknowledgements</h2>
<p>We would like to thank Douglas Eck, Geoffrey Hinton, Anja Austermann, Jeff Dean, Luke Metz, Ben Poole, Jean-Baptiste Mouret, Michiel Adriaan Unico Bacchiani, Heiga Zen, and Alexander M. Lamb for their thoughtful feedback.</p>
<p>The experiments in this work were performed on 96-core CPU Linux virtual machines provided by <a href="https://cloud.google.com/">Google Cloud Platform</a>.</p>
<p>This article was prepared using the <a href="https://distill.pub">Distill</a> <a href="https://github.com/distillpub/template">template</a>. Interactive demos were built with <a href="https://p5js.org">p5.js</a>.</p>
<p>Any errors here are our own and do not reflect opinions of our proofreaders and colleagues. If you see mistakes or want to suggest changes, feel free to contribute feedback by participating in the discussion <a href="https://github.com/weightagnostic/weightagnostic.github.io/issues">forum</a> for this article.</p>
<h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">Adam Gaier and David Ha, "Weight Agnostic Neural Networks", 2019.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{wann2019,
  author = {Adam Gaier and David Ha},
  title  = {Weight Agnostic Neural Networks},
  eprint = {arXiv:1906.04358},
  url    = {https://weightagnostic.github.io},
  note   = "\url{https://weightagnostic.github.io}",
  year   = {2019}
}</pre>
<h2>Open Source Code</h2>
<p>We release a general purpose tool, not only to facilitate reproduction, but also for further research in this direction. Our NumPy <dt-cite key="van2011numpy"></dt-cite> implementation of NEAT <dt-cite key="neat"></dt-cite> supports MPI <dt-cite key="mpi_library"></dt-cite> and OpenAI Gym <dt-cite key="openai_gym"></dt-cite> environments.</p>
<p>Please see our <a href="https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease">repo</a> for details about the code release.</p>
<h2>Reuse</h2>
<p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a href="https://github.com/weightagnostic/weightagnostic.github.io">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by the citations in their caption.</p>
<h2>Supplementary Materials</h2>
<p>For further discussion about the implementation details of the experiments, and results for multiple independent runs of the search algorithms, please refer to the Supplementary Materials section in the <a href="https://arxiv.org/abs/1906.04358">pdf</a> version of this article.</p>
<h2>Performance Profiles</h2>
<h3>Average Performance (100 trial) versus Weight for Champion Networks</h3>
<div style="text-align: center;">
<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/champ_swingup_profile.png" style="display: block; margin: auto; width: 60%;"/>
<figcaption style="text-align: center;">
CartpoleSwingUp
</figcaption>
<br/>
<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/champ_biped_profile.png" style="display: block; margin: auto; width: 60%;"/>
<figcaption style="text-align: center;">
BipedalWalker-v2
</figcaption>
<br/>
<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/champ_carracing_profile.png" style="display: block; margin: auto; width: 60%;"/>
<figcaption style="text-align: center;">
CarRacing-v0
</figcaption>
</div>
<h2>Additional Bipedal Walker Results</h2>
<h3>Increasing the search space</h3>
<div style="text-align: center;">
<img src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/png/biped_net_outConns.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">If we allow connection between outputs (a small modification to the search space), we discovered a simple and elegant WANN for the Bipedal Walker task. This particular network notably ignores many LIDAR, angle, and other input signals that are not required for the task. Refer to section on <i>Performance and Complexity</i> in the main text.</figcaption>
</div>
<div style="text-align: center;">
<video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/mp4/trial_outConns_-1.0.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">Rollout of policy using above network, weight set to -1.0. Gait is simpler compared to the network in the main text, possibly due to network's simplicity.</figcaption>
</div>
<h3>Bloopers</h3>
<div style="text-align: center;">
<video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/mp4/trial_biped_failures.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">Failure cases at bad weight values.</figcaption>
</div>
<div style="text-align: center;">
<video class="b-lazy" data-src="https://storage.googleapis.com/quickdraw-models/sketchRNN/wann/mp4/trial_balancer.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">But even at some bad weights (here, weight set to +1.14), our agent performs non trivial actions like balancing.</figcaption>
</div>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
@article{openai_gym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016},
  url = {https://arxiv.org/abs/1606.01540},
}
@article{bbc_islands,
    title={Iguana vs Snakes},
    journal={Planet Earth II, Episode 1},
    publisher={BBC Natural History},
    author={Elizabeth White},
    year=2016,
    url={https://youtu.be/Rv9hn4IGofM}
}
@article{miles1995morphological,
  title={Morphological correlates of locomotor performance in hatchling Amblyrhynchus cristatus},
  author={Miles, Donald B and Fitzgerald, Lee A and Snell, Howard L},
  journal={Oecologia},
  volume={103},
  number={2},
  pages={261--264},
  year={1995},
  url={http://tiny.cc/t5iv7y},
  publisher={Springer}
}
@article{burger1998antipredator,
  title={Antipredator behaviour of hatchling snakes: effects of incubation temperature and simulated predators},
  author={Burger, Joanna},
  journal={Animal Behaviour},
  volume={56},
  number={3},
  pages={547--553},
  year={1998},
  publisher={Elsevier}
}
@article{mori2000does,
  title={Does prey matter? Geographic variation in antipredator responses of hatchlings of a Japanese natricine snake (Rhabdophis tigrinus).},
  author={Mori, Akira and Burghardt, Gordon M},
  journal={Journal of Comparative Psychology},
  volume={114},
  number={4},
  pages={408},
  year={2000},
  url={http://tiny.cc/r8iv7y},
  publisher={American Psychological Association}
}
@article{starck1998patterns,
  title={Patterns of development: the altricial-precocial spectrum},
  author={Starck, J Matthias and Ricklefs, Robert E},
  journal={Oxford Ornithology Series},
  volume={8},
  pages={3--30},
  year={1998},
  publisher={Oxford University Press}
}
@article{goth2001innate,
  title={Innate predator-recognition in Australian brush-turkey (Alectura lathami, Megapodiidae) hatchlings},
  author={Goth, A},
  journal={Behaviour},
  volume={138},
  number={1},
  pages={117},
  year={2001},
  publisher={Leiden, Netherlands: EJ Brill, 1947-}
}
@article{lecun1995convolutional,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  url={http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf},
  year={1995}
}
@article{fukushima1982neocognitron,
  title={Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position},
  author={Fukushima, Kunihiko and Miyake, Sei},
  journal={Pattern recognition},
  volume={15},
  number={6},
  pages={455--469},
  year={1982},
  url={http://www.cs.cmu.edu/afs/cs/user/bhiksha/WWW/courses/deeplearning/Fall.2016/pdfs/Fukushima_Miyake.pdf},
  publisher={Elsevier}
}
@inproceedings{cohen2016inductive,
  title={Inductive bias of deep convolutional networks through pooling geometry},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017},
  url={https://openreview.net/forum?id=BkVsEMYel},
}
@inproceedings{he2016powerful,
  title={A powerful generative model using random weights for the deep image representation},
  author={He, Kun and Wang, Yan and Hopcroft, John},
  booktitle={Advances in Neural Information Processing Systems},
  pages={631--639},
  url={https://arxiv.org/abs/1606.04801},
  year={2016}
}
@inproceedings{ulyanov2018deep,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9446--9454},
  url={https://dmitryulyanov.github.io/deep_image_prior},
  year={2018}
}
@article{evolino,
  title={Training recurrent networks by evolino},
  author={Schmidhuber, J{\"u}rgen and Wierstra, Daan and Gagliolo, Matteo and Gomez, Faustino},
  journal={Neural computation},
  volume={19},
  number={3},
  pages={757--779},
  year={2007},
  url={http://people.idsia.ch/~juergen/evolino.html},
  publisher={MIT Press}
}
@article{jaeger2004harnessing,
  title={Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication},
  author={Jaeger, Herbert and Haas, Harald},
  journal={science},
  volume={304},
  number={5667},
  pages={78--80},
  year={2004},
  url={http://tiny.cc/t3wd8y},
  publisher={American Association for the Advancement of Science}
}
@misc{reservoir,
  title={Introduction to Reservoir Computing Methods},
  author={Roli, Andrea and Melandri, Luca},
  url={https://amslaurea.unibo.it/8268/1/melandri_luca_tesi.pdf},
  year={2014}
}
@article{lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  url={http://people.idsia.ch/~juergen/rnn.html},
  publisher={MIT Press}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  url={https://arxiv.org/abs/1706.03762},
  year={2017}
}
@inproceedings{sabour2017dynamic,
  title={Dynamic routing between capsules},
  author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={3856--3866},
  url={https://arxiv.org/abs/1710.09829},
  year={2017}
}
@book{turing1948intelligent,
  title={Intelligent machinery},
  author={Turing, Alan Mathison},
  year={1948},
  url={https://weightagnostic.github.io/papers/turing1948.pdf},
  publisher={NPL. Mathematics Division}
}
@inproceedings{harp1990designing,
  title={Designing application-specific neural networks using the genetic algorithm},
  author={Harp, Steven A and Samad, Tariq and Guha, Aloke},
  booktitle={Advances in neural information processing systems},
  pages={447--454},
  url={https://papers.nips.cc/paper/263-designing-application-specific-neural-networks-using-the-genetic-algorithm.pdf},
  year={1990}
}
@inproceedings{dasgupta1992designing,
  title={Designing application-specific neural networks using the structured genetic algorithm},
  author={Dasgupta, Dipankar and McGregor, Douglas R},
  booktitle={[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks},
  pages={87--96},
  year={1992},
  url={http://tiny.cc/2ggv7y},
  organization={IEEE}
}
@inproceedings{fullmer1992using,
  title={Using marker-based genetic encoding of neural networks to evolve finite-state behaviour},
  author={Fullmer, Brad and Miikkulainen, Risto},
  booktitle={Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life},
  pages={255--262},
  year={1992},
  url={http://tiny.cc/lggv7y},
  organization={MIT Press}
}
@inproceedings{mandischer1993representation,
  title={Representation and evolution of neural networks},
  author={Mandischer, Martin},
  booktitle={Artificial Neural Nets and Genetic Algorithms},
  pages={643--649},
  year={1993},
  url={http://tiny.cc/ofgv7y},
  organization={Springer}
}
@article{zhang1993evolving,
  title={Evolving optimal neural networks using genetic algorithms with Occam's razor},
  author={Zhang, Byoung-Tak and Muhlenbein, Heinz},
  journal={Complex systems},
  volume={7},
  number={3},
  pages={199--220},
  year={1993},
  url={http://muehlenbein.org/gpevolv93.pdf},
  publisher={[Champaign, IL, USA: Complex Systems Publications, Inc., c1987-}
}
@article{maniezzo1994genetic,
  title={Genetic evolution of the topology and weight distribution of neural networks},
  author={Maniezzo, Vittorio},
  journal={IEEE Transactions on neural networks},
  volume={5},
  number={1},
  pages={39--53},
  year={1994},
  url={http://tiny.cc/mhgv7y},
  publisher={IEEE}
}
@article{angeline1994evolutionary,
  title={An evolutionary algorithm that constructs recurrent neural networks},
  author={Angeline, Peter J and Saunders, Gregory M and Pollack, Jordan B},
  journal={IEEE transactions on Neural Networks},
  volume={5},
  number={1},
  pages={54--65},
  year={1994},
  url={http://tiny.cc/3hgv7y},
  publisher={IEEE}
}
@article{opitz1997connectionist,
  title={Connectionist theory refinement: Genetically searching the space of network topologies},
  author={Opitz, David W and Shavlik, Jude W},
  journal={Journal of Artificial Intelligence Research},
  volume={6},
  pages={177--209},
  url={http://tiny.cc/oigv7y},
  year={1997}
}
@article{pujol1998evolving,
  title={Evolving the topology and the weights of neural networks using a dual representation},
  author={Pujol, Jo{\~a}o Carlos Figueira and Poli, Riccardo},
  journal={Applied Intelligence},
  volume={8},
  number={1},
  pages={73--84},
  year={1998},
  url={http://tiny.cc/uigv7y},
  publisher={Springer}
}
@article{yao1998towards,
  title={Towards designing artificial neural networks by evolution},
  author={Yao, Xin and Liu, Yong},
  journal={Applied Mathematics and Computation},
  volume={91},
  number={1},
  pages={83--90},
  year={1998},
  url={http://tiny.cc/gjgv7y},
  publisher={Elsevier}
}
@inproceedings{lee1996evolutionary,
  title={Evolutionary ordered neural network with a linked-list encoding scheme},
  author={Lee, Chi-Ho and Kim, Jong-Hwan},
  booktitle={Proceedings of IEEE International Conference on Evolutionary Computation},
  pages={665--669},
  year={1996},
  organization={IEEE}
}
@inproceedings{gruau1996comparison,
  title={A comparison between cellular encoding and direct encoding for genetic neural networks},
  author={Gruau, Fr{\'e}d{\'e}ric and Whitley, Darrell and Pyeatt, Larry},
  booktitle={Proceedings of the 1st annual conference on genetic programming},
  pages={81--89},
  year={1996},
  organization={MIT Press}
}
@inproceedings{krishnan1994delta,
  title={Delta-gann: A new approach to training neural networks using genetic algorithms},
  author={Krishnan, Rajendra and Ciesielski, Victor B},
  booktitle={University of Queensland},
  year={1994},
  organization={Citeseer}
}
@inproceedings{braun1993evolving,
  title={Evolving feedforward neural networks},
  author={Braun, Heinrich and Weisbrod, Joachim},
  booktitle={Proceedings of ANNGA93, International Conference on Artificial Neural Networks and Genetic Algorithms},
  pages={25--32},
  year={1993},
  organization={Springer Berlin}
}
@article{neat,
  title={Evolving neural networks through augmenting topologies},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Evolutionary computation},
  volume={10},
  number={2},
  pages={99--127},
  year={2002},
  url={http://www.cs.ucf.edu/~kstanley/neat.html},
  publisher={MIT Press}
}
@inproceedings{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017},
  url={https://openreview.net/forum?id=r1Ue8Hcxg},
}
@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2902--2911},
  year={2017},
  url={https://arxiv.org/abs/1703.01041},
  organization={JMLR. org}
}
@inproceedings{liu2017hierarchical,
  title={Hierarchical Representations for Efficient Architecture Search},
  author={Hanxiao Liu and Karen Simonyan and Oriol Vinyals and Chrisantha Fernando and Koray Kavukcuoglu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://openreview.net/forum?id=BJQRKzbA-},
}
@incollection{miikkulainen2019evolving,
  title={Evolving deep neural networks},
  author={Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Daniel and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel},
  booktitle={Artificial Intelligence in the Age of Neural Networks and Brain Computing},
  pages={293--312},
  year={2019},
  url={https://arxiv.org/abs/1703.00548},
  publisher={Elsevier}
}
@inproceedings{jozefowicz2015empirical,
  title={An empirical exploration of recurrent network architectures},
  author={Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={2342--2350},
  url={http://proceedings.mlr.press/v37/jozefowicz15.pdf},
  year={2015}
}
@inproceedings{so2019evolved,
  title={The Evolved Transformer},
  author={So, David R and Liang, Chen and Le, Quoc V},
  booktitle={International Conference on Machine Learning},
  url={https://arxiv.org/abs/1901.11117},
  year={2019}
}
@article{li2019random,
  title={Random search and reproducibility for neural architecture search},
  author={Li, Liam and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1902.07638},
  url={https://arxiv.org/abs/1902.07638},
  year={2019}
}
@article{sciuto2019evaluating,
  title={Evaluating the Search Phase of Neural Architecture Search},
  author={Sciuto, Christian and Yu, Kaicheng and Jaggi, Martin and Musat, Claudiu and Salzmann, Mathieu},
  journal={arXiv preprint arXiv:1902.08142},
  url={https://arxiv.org/abs/1902.08142},
  year={2019}
}
@inproceedings{real2018regularized,
  title={Regularized evolution for image classifier architecture search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
  year={2019},
  url={https://arxiv.org/abs/1802.01548},
}
@inproceedings{pham2018efficient,
  title={Efficient Neural Architecture Search via Parameter Sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International Conference on Machine Learning},
  pages={4092--4101},
  url={https://arxiv.org/abs/1802.03268},
  year={2018}
}
@inproceedings{brock2017smash,
  title={SMASH: One-shot model architecture search through hypernetworks},
  author={Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://openreview.net/forum?id=rydeCEhs-},
}
@inproceedings{
  liu2018darts,
  title={DARTS: Differentiable Architecture Search},
  author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://openreview.net/forum?id=S1eYHoC5FX},
}
@article{hinton1996learning,
  title={How learning can guide evolution},
  author={Hinton, Geoffrey E and Nowlan, Steven J},
  journal={Adaptive individuals in evolving populations: models and algorithms},
  volume={26},
  pages={447--454},
  year={1996},
  url={http://www.cogsci.ucsd.edu/~rik/courses/cogs184_w10/readings/HintonNowlan97.pdf},
  publisher={Addison-Wesley Reading, MA}
}
@article{smith1987learning,
  title={When learning guides evolution},
  author={Smith, John Maynard},
  journal={Nature},
  volume={329},
  number={6142},
  pages={761},
  year={1987},
  url={https://www.nature.com/articles/329761a0.pdf},
  publisher={Nature Publishing Group}
}
@article{baldwin1896new,
  title={A new factor in evolution},
  author={Baldwin, J Mark},
  journal={The american naturalist},
  volume={30},
  number={354},
  pages={441--451},
  year={1896},
  url={https://en.wikipedia.org/wiki/Baldwin_effect},
  publisher={Edwards and Docker}
}
@article{mackay1992bayesian,
  title={Bayesian interpolation},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={415--447},
  year={1992},
  url={https://authors.library.caltech.edu/13792/1/MACnc92a.pdf},
  publisher={MIT Press}
}
@article{barber1998ensemble,
  title={Ensemble learning in Bayesian neural networks},
  author={Barber, D and Bishop, CM},
  journal={NATO ASI series. Series F: computer and system sciences},
  pages={215--237},
  year={1998},
  url={http://tiny.cc/8jgv7y},
  publisher={Plenum}
}
@article{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  url={https://aka.ms/prml},
  publisher={Springer (but now offered as free download, see url.)}
}
@article{neal2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  year={2012},
  url={http://tiny.cc/vkgv7y},
  publisher={Springer Science and Business Media}
}
@phdthesis{gal2016uncertainty,
  title={Uncertainty in deep learning},
  author={Gal, Yarin},
  year={2016},
  url={http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf},
  school={PhD thesis, University of Cambridge}
}
@inproceedings{hanson1990meiosis,
  title={Meiosis networks},
  author={Hanson, Stephen Jos{\'e}},
  booktitle={Advances in neural information processing systems},
  pages={533--541},
  url={http://papers.nips.cc/paper/227-meiosis-networks.pdf},
  year={1990}
}
@article{hanson1990stochastic,
  title={A stochastic version of the delta rule},
  author={Hanson, Stephen Jos{\'e}},
  journal={Physica D: Nonlinear Phenomena},
  volume={42},
  number={1-3},
  pages={265--272},
  year={1990},
  url={https://github.com/theSage21/stochastic_delta_rule},
  publisher={Elsevier}
}
@inproceedings{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  booktitle={Advances in neural information processing systems},
  pages={2348--2356},
  url={https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks},
  year={2011}
}
@article{krueger2017bayesian,
  title={Bayesian hypernetworks},
  author={Krueger, David and Huang, Chin-Wei and Islam, Riashat and Turner, Ryan and Lacoste, Alexandre and Courville, Aaron},
  journal={arXiv preprint arXiv:1710.04759},
  url={https://arxiv.org/abs/1710.04759},
  year={2017}
}
@inproceedings{neklyudov2018variance,
  title={Variance Networks: When Expectation Does Not Meet Your Expectations},
  author={Kirill Neklyudov and Dmitry Molchanov and Arsenii Ashukha and Dmitry Vetrov},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://openreview.net/forum?id=B1GAUs0cKQ},
}
@article{solomonoff1964formal,
  title={A formal theory of inductive inference. Part I},
  author={Solomonoff, Ray J},
  journal={Information and control},
  volume={7},
  number={1},
  pages={1--22},
  year={1964},
  url={http://tiny.cc/7cgv7y},
  publisher={Elsevier}
}
@article{kolmogorov1965three,
  title={Three approaches to the quantitative definition of information},
  author={Kolmogorov, Andrei N},
  journal={Problems of information transmission},
  volume={1},
  number={1},
  pages={1--7},
  url={http://tiny.cc/xmgv7y},
  year={1965}
}
@article{rissanen1978modeling,
  title={Modeling by shortest data description},
  author={Rissanen, Jorma},
  journal={Automatica},
  volume={14},
  number={5},
  pages={465--471},
  year={1978},
  url={http://tiny.cc/dngv7y},
  publisher={Elsevier}
}
@book{grunwald2007minimum,
  title={The minimum description length principle},
  author={Gr{\"u}nwald, Peter D},
  year={2007},
  publisher={MIT press},
  url={https://mitpress.mit.edu/books/minimum-description-length-principle},
}
@book{rissanen2007information,
  title={Information and complexity in statistical modeling},
  author={Rissanen, Jorma},
  year={2007},
  publisher={Springer Science and Business Media},
  url={https://www.springer.com/gp/book/9780387366104},
}
@article{nowlan1992simplifying,
  title={Simplifying neural networks by soft weight-sharing},
  author={Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={4},
  number={4},
  pages={473--493},
  year={1992},
  url={http://www.cs.toronto.edu/~hinton/absps/sunspots.pdf},
  publisher={MIT Press}
}
@inproceedings{hinton1993keeping,
  title={Keeping neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey and Van Camp, Drew},
  booktitle={In Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory},
  year={1993},
  url={http://www.cs.toronto.edu/~fritz/absps/colt93.pdf},
  organization={Citeseer}
}
@article{schmidhuber1997discovering,
  title={Discovering neural nets with low Kolmogorov complexity and high generalization capability},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={10},
  number={5},
  pages={857--873},
  year={1997},
  url={ftp://ftp.idsia.ch/pub/juergen/loconet.pdf},
  publisher={Elsevier}
}
@inproceedings{blier2018description,
  title={The description length of deep learning models},
  author={Blier, L{\'e}onard and Ollivier, Yann},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2216--2226},
  url={https://arxiv.org/abs/1802.07044},
  year={2018}
}
@inproceedings{li2018measuring,
  title={Measuring the Intrinsic Dimension of Objective Landscapes},
  author={Chunyuan Li and Heerad Farkhoor and Rosanne Liu and Jason Yosinski},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
  url={https://openreview.net/forum?id=ryup8-WCW},
}
@inproceedings{trask2018neural,
  title={Neural arithmetic logic units},
  author={Trask, Andrew and Hill, Felix and Reed, Scott E and Rae, Jack and Dyer, Chris and Blunsom, Phil},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8035--8044},
  url={https://arxiv.org/abs/1808.00508},
  year={2018}
}
@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  url={http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf},
  year={1990}
}
@inproceedings{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  booktitle={Advances in neural information processing systems},
  pages={164--171},
  url={http://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf},
  year={1993}
}
@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  url={https://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf},
  year={2015}
}
@inproceedings{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  booktitle={Advances In Neural Information Processing Systems},
  pages={1379--1387},
  url={http://papers.nips.cc/paper/6165-dynamic-network-surgery-for-efficient-dnns.pdf},
  year={2016}
}
@inproceedings{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017},
  url={https://openreview.net/forum?id=rJqFGTslg},
}
@inproceedings{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017},
  url={https://openreview.net/forum?id=SJGCiw5gl},
}
@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5058--5066},
  url={https://arxiv.org/abs/1707.06342},
  year={2017}
}
@inproceedings{liu2018rethinking,
  title={Rethinking the Value of Network Pruning},
  author={Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://openreview.net/forum?id=rJlnB3C5Ym},
}
@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Jonathan Frankle and Michael Carbin},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://openreview.net/forum?id=rJl-b3RcF7},
}
@inproceedings{lee2018snip,
  title={SNIP: Single-shot Network Pruning based on Connection Sensitivity},
  author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://openreview.net/forum?id=B1VZqjAcYX},
}
@article{zhou2019deconstructing,
  title={Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
  author={Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  journal={arXiv preprint arXiv:1905.01067},
  url={https://arxiv.org/abs/1905.01067},
  year={2019}
}
@inproceedings{mallya2018piggyback,
  title={Piggyback: Adapting a single network to multiple tasks by learning to mask weights},
  author={Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={67--82},
  url={https://arxiv.org/abs/1801.06519},
  year={2018}
}
@book{seung2012connectome,
  title={Connectome: How the brain's wiring makes us who we are},
  author={Seung, Sebastian},
  year={2012},
  url={https://en.wikipedia.org/wiki/Connectome_(book)},
  publisher={HMH}
}
@misc{seung2012ted,
    title = {I am my connectome},
    year = {2010},
    journal = {TED Talk},
    author = {Seung, Sebastian},
    url = {https://www.ted.com/talks/sebastian_seung},
}
@article{sporns2005human,
  title={The human connectome: a structural description of the human brain},
  author={Sporns, Olaf and Tononi, Giulio and K{\"o}tter, Rolf},
  journal={PLoS computational biology},
  volume={1},
  number={4},
  pages={e42},
  year={2005},
  url={https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0010042},
  publisher={Public Library of Science}
}
@article{white1986structure,
  title={The structure of the nervous system of the nematode Caenorhabditis elegans},
  author={White, John G and Southgate, Eileen and Thomson, J Nichol and Brenner, Sydney},
  journal={Philos Trans R Soc Lond B Biol Sci},
  volume={314},
  number={1165},
  pages={1--340},
  url={http://tiny.cc/kiiv7y},
  year={1986}
}
@article{varshney2011structural,
  title={Structural properties of the Caenorhabditis elegans neuronal network},
  author={Varshney, Lav R and Chen, Beth L and Paniagua, Eric and Hall, David H and Chklovskii, Dmitri B},
  journal={PLoS computational biology},
  volume={7},
  number={2},
  pages={e1001066},
  year={2011},
  url={https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066},
  publisher={Public Library of Science}
}
@article{eichler2017complete,
  title={The complete connectome of a learning and memory centre in an insect brain},
  author={Eichler, Katharina and Li, Feng and Litwin-Kumar, Ashok and Park, Youngser and Andrade, Ingrid and Schneider-Mizell, Casey M and Saumweber, Timo and Huser, Annina and Eschbach, Claire and Gerber, Bertram},
  journal={Nature},
  volume={548},
  number={7666},
  pages={175},
  year={2017},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5806122/},
  publisher={Nature Publishing Group}
}
@article{takemura2017connectome,
  title={A connectome of a learning and memory center in the adult Drosophila brain},
  author={Takemura, Shin-ya and Aso, Yoshinori and Hige, Toshihide and Wong, Allan and Lu, Zhiyuan and Xu, C Shan and Rivlin, Patricia K and Hess, Harald and Zhao, Ting and Parag, Toufiq},
  journal={Elife},
  volume={6},
  pages={e26975},
  year={2017},
  url={https://elifesciences.org/articles/26975},
  publisher={eLife Sciences Publications Limited}
}
@article{huttenlocher1990morphometric,
  title={Morphometric study of human cerebral cortex development},
  author={Huttenlocher, Peter R},
  journal={Neuropsychologia},
  volume={28},
  number={6},
  pages={517--527},
  year={1990},
  publisher={Elsevier}
}
@article{tierney2009brain,
  title={Brain development and the role of experience in the early years},
  author={Tierney, Adrienne L and Nelson III, Charles A},
  journal={Zero to three},
  volume={30},
  number={2},
  pages={9},
  year={2009},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3722610/},
  publisher={NIH Public Access}
}
@article{black1990learning,
  title={Learning causes synaptogenesis, whereas motor activity causes angiogenesis, in cerebellar cortex of adult rats.},
  author={Black, James E and Isaacs, Krystyna R and Anderson, Brenda J and Alcantara, Adriana A and Greenough, William T},
  journal={Proceedings of the National Academy of Sciences},
  volume={87},
  number={14},
  pages={5568--5572},
  year={1990},
  url={https://www.pnas.org/content/pnas/87/14/5568.full.pdf},
  publisher={National Acad Sciences}
}
@article{bruer1999neural,
  title={Neural connections: Some you use, some you lose},
  author={Bruer, John T},
  journal={The Phi Delta Kappan},
  volume={81},
  number={4},
  pages={264--277},
  year={1999},
  url={http://www.oecd.org/education/ceri/31709587.pdf},
  publisher={JSTOR}
}
@article{kleim2002motor,
  title={Motor learning-dependent synaptogenesis is localized to functionally reorganized motor cortex},
  author={Kleim, Jeffrey A and Barbay, Scott and Cooper, Natalie R and Hogg, Theresa M and Reidel, Chelsea N and Remple, Michael S and Nudo, Randolph J},
  journal={Neurobiology of learning and memory},
  volume={77},
  number={1},
  pages={63--77},
  year={2002},
  publisher={Elsevier}
}
@article{dayan2011neuroplasticity,
  title={Neuroplasticity subserving motor skill learning},
  author={Dayan, Eran and Cohen, Leonardo G},
  journal={Neuron},
  volume={72},
  number={3},
  pages={443--454},
  year={2011},
  url={http://tiny.cc/1ziv7y},
  publisher={Elsevier}
}
@article{bullmore2009complex,
  title={Complex brain networks: graph theoretical analysis of structural and functional systems},
  author={Bullmore, Ed and Sporns, Olaf},
  journal={Nature reviews neuroscience},
  volume={10},
  number={3},
  pages={186},
  year={2009},
  url={http://tiny.cc/xyiv7y},
  publisher={Nature Publishing Group}
}
@article{he2010graph,
  title={Graph theoretical modeling of brain connectivity},
  author={He, Yong and Evans, Alan},
  journal={Current opinion in neurology},
  volume={23},
  number={4},
  pages={341--350},
  year={2010},
  url={http://tiny.cc/82iv7y},
  publisher={LWW}
}
@article{van2011rich,
  title={Rich-club organization of the human connectome},
  author={Van Den Heuvel, Martijn P and Sporns, Olaf},
  journal={Journal of Neuroscience},
  volume={31},
  number={44},
  pages={15775--15786},
  year={2011},
  url={http://www.jneurosci.org/content/jneuro/31/44/15775.full.pdf},
  publisher={Soc Neuroscience}
}
@article{tournamentSelection,
  title={Genetic algorithms, tournament selection, and the effects of noise},
  author={Miller, Brad L and Goldberg, David E},
  journal={Complex systems},
  volume={9},
  number={3},
  pages={193--212},
  year={1995},
  url={http://tiny.cc/4ckv7y},
  publisher={[Champaign, IL, USA: Complex Systems Publications, Inc., c1987-}
}
@incollection{mouret2011novelty,
  title={Novelty-based multiobjectivization},
  author={Mouret, Jean-Baptiste},
  booktitle={New horizons in evolutionary robotics},
  pages={139--154},
  year={2011},
  url={http://tiny.cc/uelv7y},
  publisher={Springer}
}
@article{konak2006multi,
  title={Multi-objective optimization using genetic algorithms: A tutorial},
  author={Konak, Abdullah and Coit, David W and Smith, Alice E},
  journal={Reliability Engineering and System Safety},
  volume={91},
  number={9},
  pages={992--1007},
  year={2006},
  url={http://tiny.cc/bglv7y},
  publisher={Elsevier}
}
@article{clune2013evolutionary,
  title={The evolutionary origins of modularity},
  author={Clune, Jeff and Mouret, Jean-Baptiste and Lipson, Hod},
  journal={Proceedings of the Royal Society b: Biological sciences},
  volume={280},
  number={1755},
  year={2013},
  url={https://royalsocietypublishing.org/doi/pdf/10.1098/rspb.2012.2863},
  publisher={The Royal Society}
}
@article{nsga2,
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II},
  author={Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
  journal={IEEE transactions on evolutionary computation},
  volume={6},
  number={2},
  pages={182--197},
  year={2002},
  url={http://www.dmi.unict.it/mpavone/nc-cs/materiale/NSGA-II.pdf},
  publisher={IEEE}
}
@article{tedrake2009underactuated,
  title={Underactuated Robotics: Learning, Planning, and Control for Efficient and Agile Machines: Course Notes for MIT 6.832},
  author={Tedrake, Russ},
  journal={Working draft edition},
  volume={3},
  year={2009},
  url={http://tiny.cc/v8lv7y},
  publisher={Citeseer}
}
@article{raiko2009variational,
  title={Variational Bayesian learning of nonlinear hidden state-space models for model predictive control},
  author={Raiko, T and Tornio, M},
  journal={Neurocomputing},
  year={2009},
  url={https://users.ics.aalto.fi/praiko/papers/raikotornio2009.pdf},
  publisher={Elsevier}
}
@inproceedings{gal2016improving,
  title={Improving PILCO with Bayesian neural network dynamics models},
  author={Gal, Yarin and McAllister, Rowan and Rasmussen, Carl Edward},
  booktitle={Data-Efficient Machine Learning workshop, ICML},
  volume={4},
  url={http://mlg.eng.cam.ac.uk/yarin/website/PDFs/DeepPILCO.pdf},
  year={2016}
}
@article{deepPILCOgithub,
  author = {Zuo, Xingdong},
  title = {PyTorch implementation of Improving PILCO with Bayesian neural network dynamics models},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/zuoxingdong/DeepPILCO},
}
@incollection{ha2018worldmodels,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages = {2451--2463},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  url = {https://worldmodels.github.io},
}
@inproceedings{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2014},
  url={https://openreview.net/forum?id=33X9fd2-9FyZd},
}
@inproceedings{vae_dm,
  title={Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle={International Conference on Machine Learning},
  pages={1278--1286},
  url={https://arxiv.org/abs/1401.4082},
  year={2014}
}
@article{ha2018designrl,
  author = {David Ha},
  title  = {Reinforcement Learning for Improving Agent Design},
  journal = {arXiv:1810.03779},
  url    = {https://designrl.github.io},
  year   = {2018}
}
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  url={http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf},
  publisher={Springer}
}
@misc{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann},
  url={http://yann.lecun.com/exdb/mnist/},
  year={1998}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  url={http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
  year={1998},
}
@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran{\c{c}}ois},
  url={https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py},
  year={2015}
}
@article{parisi2018continual,
  title={Continual Lifelong Learning with Neural Networks: A Review},
  author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal={arXiv preprint arXiv:1802.07569},
  url={https://arxiv.org/abs/1802.07569},
  year={2018}
}
@article{zador2019critique,
  title={A Critique of Pure Learning: What Artificial Neural Networks can Learn from Animal Brains},
  author={Zador, Anthony M},
  journal={BioRxiv},
  year={2019},
  url={https://www.biorxiv.org/content/10.1101/582643v1},
  publisher={Cold Spring Harbor Laboratory}
}
@article{ackley1991interactions,
  title={Interactions between learning and evolution},
  author={Ackley, David and Littman, Michael},
  journal={Artificial life II},
  volume={10},
  pages={487--509},
  year={1991},
  url={http://www2.hawaii.edu/~nreed/ics606/papers/Ackley91learningEvolution.pdf},
}
@inproceedings{schmidhuber1991curious,
  title={Curious model-building control systems},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={[Proceedings] 1991 IEEE International Joint Conference on Neural Networks},
  pages={1458--1463},
  year={1991},
  url={http://people.idsia.ch/~juergen/interest.html},
  organization={IEEE}
}
@article{oudeyer2007intrinsic,
  title={Intrinsic motivation systems for autonomous mental development},
  author={Oudeyer, Pierre-Yves and Kaplan, Frdric and Hafner, Verena V},
  journal={IEEE transactions on evolutionary computation},
  volume={11},
  number={2},
  pages={265--286},
  year={2007},
  url={http://www.pyoudeyer.com/ims.pdf},
  publisher={IEEE}
}
@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  url={https://pathak22.github.io/noreward-rl/},
  year={2017}
}
@inproceedings{lehman2008exploiting,
  title={Exploiting open-endedness to solve problems through the search for novelty.},
  author={Lehman, Joel and Stanley, Kenneth O},
  booktitle={ALIFE},
  pages={329--336},
  url={https://www.cs.ucf.edu/eplex/noveltysearch/userspage/},
  year={2008}
}
@inproceedings{jang2016categorical,
  title={Categorical Reparameterization with Gumbel-Softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017},
  url={https://openreview.net/forum?id=rkE3y85ee},
}
@article{graves2014neural,
  title={Neural Turing Machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  url={https://arxiv.org/abs/1410.5401},
  year={2014}
}
@article{graves2016adaptive,
  title={Adaptive computation time for recurrent neural networks},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1603.08983},
  url={https://arxiv.org/abs/1603.08983},
  year={2016}
}
@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  url={https://arxiv.org/abs/1703.03400},
  organization={JMLR. org}
}
@article{van2011numpy,
  title={The NumPy array: a structure for efficient numerical computation},
  author={Van Der Walt, Stefan and Colbert, S Chris and Varoquaux, Gael},
  journal={Computing in Science and Engineering},
  volume={13},
  number={2},
  pages={22},
  year={2011},
  publisher={IEEE Computer Society},
  url={https://www.numpy.org/},
}
@book{mpi_library,
  title={Using MPI: portable parallel programming with the message-passing interface},
  author={Gropp, William D and Gropp, William and Lusk, Ewing and Skjellum, Anthony},
  year={1999},
  publisher={MIT press},
  url={https://www.mpi-forum.org/},
}
</script>
<!--


-->
<script language="javascript" type="text/javascript" src="lib/p5.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/p5.dom.js"></script>
<script language="javascript" type="text/javascript" src="lib/numjs.js"></script>
<script language="javascript" type="text/javascript" src="lib/agent.js"></script>
<script language="javascript" type="text/javascript" src="lib/wann_agent.js"></script>
<script language="javascript" type="text/javascript" src="lib/swingup.js"></script>
<script src="lib/blazy.js"></script>
<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/demo.js"></script>
<script language="javascript" type="text/javascript" src="lib/controller.js"></script>
